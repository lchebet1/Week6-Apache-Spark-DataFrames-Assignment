{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8959b8bc",
      "metadata": {
        "id": "8959b8bc"
      },
      "source": [
        "# Instructions:\n",
        "As a Data professional, you need to perform an analysis by answering questions about\n",
        "some stock market data on Safaricom from the years 2012-2017.\n",
        "You will need to perform the following:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab8f9d6",
      "metadata": {
        "id": "dab8f9d6"
      },
      "source": [
        "# Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fe1b805d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe1b805d",
        "outputId": "3ea765cb-9725-4287-fc90-da8e0c0c8294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# import pyspark module for analysis\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark # only run after findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983695f9",
      "metadata": {
        "id": "983695f9"
      },
      "source": [
        "# Data Importation and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d06d4d8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d06d4d8b",
        "outputId": "a4fec65a-c953-4922-caef-629de5db8f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Read the csv saf_stock\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "df = sqlContext.read.load('saf_stock.csv', \n",
        "                      format='com.databricks.spark.csv', \n",
        "                      header='true', \n",
        "                      inferSchema='true')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "77158e01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77158e01",
        "outputId": "9bb7445a-5640-4111-dba9-b1495895e84a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#Determine the column names\n",
        "\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90787db",
      "metadata": {
        "id": "c90787db"
      },
      "source": [
        "#### observation: there are spaces on columns names that need to be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b777d200",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b777d200",
        "outputId": "7db33ade-08b4-4163-a54e-53d8df85c350"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj_Close']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#remove spaces on headers and replace with '_'\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df2 = df.select([F.col(col).alias(col.replace(' ', '_')) for col in df.columns])\n",
        "df2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ce670559",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce670559",
        "outputId": "90c57610-fa36-4e20-ba9a-c29af0f1fa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj_Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#‚óè Make observations about the schema.\n",
        "\n",
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d8678900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8678900",
        "outputId": "54c6aa5c-3b2d-495f-8049-46ac51c5f517"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Date=datetime.datetime(2012, 1, 3, 0, 0), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj_Close=52.619234999999996),\n",
              " Row(Date=datetime.datetime(2012, 1, 4, 0, 0), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj_Close=52.078475),\n",
              " Row(Date=datetime.datetime(2012, 1, 5, 0, 0), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj_Close=51.825539),\n",
              " Row(Date=datetime.datetime(2012, 1, 6, 0, 0), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj_Close=51.45922),\n",
              " Row(Date=datetime.datetime(2012, 1, 9, 0, 0), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj_Close=51.616215000000004)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "#Show the first 5 rows of column\n",
        "df2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d25e9a8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d25e9a8a",
        "outputId": "c1748f5f-68fc-43f6-e5d7-46c5d74c1237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['saf_stock']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#first we create an sql table from sqlcontext\n",
        "from pyspark.sql import SQLContext\n",
        "sqlCtx = SQLContext(sc)\n",
        "df2.createOrReplaceTempView('saf_stock')\n",
        "tables = sqlCtx.tableNames()\n",
        "print(tables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6a4554bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a4554bb",
        "outputId": "0c2f3805-9b4c-4e4b-a590-ce13e5931640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj_Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Use the describe method to learn about the data frame\n",
        "query = 'select Open,High, Low, Close, Volume,Adj_Close from saf_stock'\n",
        "sqlCtx.sql(query).describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66164fc8",
      "metadata": {
        "id": "66164fc8"
      },
      "source": [
        "# Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "400a6638",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "400a6638",
        "outputId": "f6b5b9cb-af86-4f6f-f1d2-20d6ade42c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-----+-----+-----+--------+---------+\n",
            "|               Date| Open| High|  Low|Close|  Volume|Adj_Close|\n",
            "+-------------------+-----+-----+-----+-----+--------+---------+\n",
            "|2012-01-03 00:00:00|59.97|61.06|59.87|60.33|12668800|    52.62|\n",
            "|2012-01-04 00:00:00|60.21|60.35|59.47|59.71| 9593300|    52.08|\n",
            "|2012-01-05 00:00:00|59.35|59.62|58.37|59.42|12768200|    51.83|\n",
            "|2012-01-06 00:00:00|59.42|59.45|58.87| 59.0| 8069400|    51.46|\n",
            "|2012-01-09 00:00:00|59.03|59.55|58.92|59.18| 6679300|    51.62|\n",
            "|2012-01-10 00:00:00|59.43|59.71|58.98|59.04| 6907300|    51.49|\n",
            "|2012-01-11 00:00:00|59.06|59.53|59.04| 59.4| 6365600|    51.81|\n",
            "|2012-01-12 00:00:00|59.79| 60.0| 59.4| 59.5| 7236400|     51.9|\n",
            "|2012-01-13 00:00:00|59.18|59.61|59.01|59.54| 7729300|    51.93|\n",
            "|2012-01-17 00:00:00|59.87|60.11|59.52|59.85| 8500000|     52.2|\n",
            "|2012-01-18 00:00:00|59.79|60.03|59.65|60.01| 5911400|    52.34|\n",
            "|2012-01-19 00:00:00|59.93|60.73|59.75|60.61| 9234600|    52.86|\n",
            "|2012-01-20 00:00:00|60.75|61.25|60.67|61.01|10378800|    53.21|\n",
            "|2012-01-23 00:00:00|60.81|60.98|60.51|60.91| 7134100|    53.13|\n",
            "|2012-01-24 00:00:00|60.75| 62.0|60.75|61.39| 7362800|    53.54|\n",
            "|2012-01-25 00:00:00|61.18|61.61|61.04|61.47| 5915800|    53.61|\n",
            "|2012-01-26 00:00:00| 61.8|61.84|60.77|60.97| 7436200|    53.18|\n",
            "|2012-01-27 00:00:00|60.86|61.12|60.54|60.71| 6287300|    52.95|\n",
            "|2012-01-30 00:00:00|60.47|61.32|60.35| 61.3| 7636900|    53.47|\n",
            "|2012-01-31 00:00:00|61.53|61.57|60.58|61.36| 9761500|    53.52|\n",
            "+-------------------+-----+-----+-----+-----+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Format all the data to 2 decimal places i.e. format_number()\n",
        "import pyspark.sql.functions as func\n",
        "\n",
        "\n",
        "df_1 = df2.withColumn(\"Open\", func.round(df2[\"Open\"], 2))\n",
        "df_2 = df_1.withColumn(\"High\", func.round(df_1[\"High\"], 2))\n",
        "df_3 = df_2.withColumn(\"Low\", func.round(df_2[\"Low\"], 2))\n",
        "df_4= df_3.withColumn(\"Close\", func.round(df_3[\"Close\"], 2))\n",
        "df_5 = df_4.withColumn(\"Volume\", func.round(df_4[\"Volume\"], 2))\n",
        "df3 = df_5.withColumn(\"Adj_Close\", func.round(df_5[\"Adj_Close\"], 2))\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "35fa93ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35fa93ce",
        "outputId": "5743da49-8142-4b73-87f7-9b1e4ed4db09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|               Date| Open| High|  Low|Close|  Volume|Adj_Close|            HV_Ratio|\n",
            "+-------------------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|2012-01-03 00:00:00|59.97|61.06|59.87|60.33|12668800|    52.62|4.819714574387472E-6|\n",
            "|2012-01-04 00:00:00|60.21|60.35|59.47|59.71| 9593300|    52.08|6.290848821573389...|\n",
            "|2012-01-05 00:00:00|59.35|59.62|58.37|59.42|12768200|    51.83|4.669413073103491E-6|\n",
            "|2012-01-06 00:00:00|59.42|59.45|58.87| 59.0| 8069400|    51.46|7.367338339901356E-6|\n",
            "|2012-01-09 00:00:00|59.03|59.55|58.92|59.18| 6679300|    51.62|8.915604928660188E-6|\n",
            "|2012-01-10 00:00:00|59.43|59.71|58.98|59.04| 6907300|    51.49|8.644477581688938E-6|\n",
            "|2012-01-11 00:00:00|59.06|59.53|59.04| 59.4| 6365600|    51.81| 9.35182857861003E-6|\n",
            "|2012-01-12 00:00:00|59.79| 60.0| 59.4| 59.5| 7236400|     51.9| 8.29141562102703E-6|\n",
            "|2012-01-13 00:00:00|59.18|59.61|59.01|59.54| 7729300|    51.93|7.712211972623653E-6|\n",
            "|2012-01-17 00:00:00|59.87|60.11|59.52|59.85| 8500000|     52.2|7.071764705882352...|\n",
            "|2012-01-18 00:00:00|59.79|60.03|59.65|60.01| 5911400|    52.34|1.015495483303447...|\n",
            "|2012-01-19 00:00:00|59.93|60.73|59.75|60.61| 9234600|    52.86|6.576354146362592...|\n",
            "|2012-01-20 00:00:00|60.75|61.25|60.67|61.01|10378800|    53.21| 5.90145296180676E-6|\n",
            "|2012-01-23 00:00:00|60.81|60.98|60.51|60.91| 7134100|    53.13|8.547679455011844E-6|\n",
            "|2012-01-24 00:00:00|60.75| 62.0|60.75|61.39| 7362800|    53.54|8.420709512685392E-6|\n",
            "|2012-01-25 00:00:00|61.18|61.61|61.04|61.47| 5915800|    53.61|1.041448324825044...|\n",
            "|2012-01-26 00:00:00| 61.8|61.84|60.77|60.97| 7436200|    53.18|8.316075414862431E-6|\n",
            "|2012-01-27 00:00:00|60.86|61.12|60.54|60.71| 6287300|    52.95|9.721183974042911E-6|\n",
            "|2012-01-30 00:00:00|60.47|61.32|60.35| 61.3| 7636900|    53.47|8.029436027707578E-6|\n",
            "|2012-01-31 00:00:00|61.53|61.57|60.58|61.36| 9761500|    53.52|6.307432259386365E-6|\n",
            "+-------------------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "####Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
        "\n",
        "\n",
        "new_df = df3.withColumn('HV_Ratio', df3.High/df3.Volume)\n",
        "new_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4eae487",
      "metadata": {
        "id": "e4eae487"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cda123f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cda123f1",
        "outputId": "7369b862-0f59-4627-b5aa-1ed1010edef5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.97"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "#What day had the Peak High in Price?\n",
        "\n",
        "#new_df.agg({'High': 'max' }).collect()\n",
        "\n",
        "#df.groupBy('address').max('height').collect()\n",
        "\n",
        "#de = new_df.agg({'High': 'max' })\n",
        "#de.show()\n",
        "#de = select High.Max\n",
        "import pandas as pd\n",
        "t = new_df.agg({'High': 'max' })\n",
        "\n",
        "x = t.collect()[0][0]\n",
        "\n",
        "x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2489db10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2489db10",
        "outputId": "cc253103-a6a5-4023-d93b-5be32616d88b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Date=datetime.datetime(2012, 1, 3, 0, 0), Open=59.97, High=61.06, Low=59.87, Close=60.33, Volume=12668800, Adj_Close=52.62, HV_Ratio=4.819714574387472e-06)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8fd12e32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fd12e32",
        "outputId": "799cabc8-e699-4d75-b375-505c1728a50d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844992050863|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#What is the mean of the Close column?\n",
        "\n",
        "from pyspark.sql.functions import avg\n",
        "new_df.select(avg(new_df.Close)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "26f1e0a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26f1e0a1",
        "outputId": "ce425215-d3d2-4b4f-ed87-c99880a46a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Volume_max|Volume_min|\n",
            "+----------+----------+\n",
            "|  80898100|   2094900|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#What is the max and min of the Volume column?\n",
        "from pyspark.sql.functions import max\n",
        "from pyspark.sql.functions import min\n",
        "new_df.select(max(new_df.Volume).alias(\"Volume_max\"), \n",
        "          min(new_df.Volume).alias(\"Volume_min\")\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "271e23c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271e23c4",
        "outputId": "f3f2bc0a-592a-498a-e4ee-400c66043d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of below 60 dollars =  81\n"
          ]
        }
      ],
      "source": [
        "#How many days was the Close lower than 60 dollars?\n",
        "from pyspark.sql.functions import count\n",
        "below_60_dolars = ''\n",
        "below_60_dolars = 'select Close from saf_stock where Close < 60'\n",
        "x = sqlCtx.sql(below_60_dolars).count()\n",
        "\n",
        "print('Counts of below 60 dollars = ', x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f3879696",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3879696",
        "outputId": "00137d6e-6c82-46ba-cd52-301a2434b4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What percentage of the time was the High greater than 80 dollars =  9.141494435612083 %\n"
          ]
        }
      ],
      "source": [
        "#What percentage of the time was the High greater than 80 dollars?\n",
        "#count the total times of High\n",
        "High_count = ''\n",
        "\n",
        "High_count = 'select High from saf_stock'\n",
        "y = sqlCtx.sql(High_count).count()\n",
        "\n",
        "#count the number of times High is greater than 80\n",
        "High_Above_80_dolars = ''\n",
        "High_Above_80_dolars = 'select High from saf_stock where High > 80'\n",
        "z = sqlCtx.sql(High_Above_80_dolars).count()\n",
        "\n",
        "# calculate the % using the occurence\n",
        "\n",
        "a = 100*(z/y)\n",
        "\n",
        "print ('What percentage of the time was the High greater than 80 dollars = ',a , '%' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "21597acf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21597acf",
        "outputId": "5f822ca9-2857-4528-c9bc-8464ff6a5a3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.33843260582148915"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#What is the Pearson correlation between High and Volume?\n",
        "\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "new_df.corr('High' , 'Volume')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3b63a3c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b63a3c6",
        "outputId": "829f8bbc-484f-45b5-bf8b-ff939e8f1bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|max(High)|\n",
            "+----+---------+\n",
            "|2016|    75.19|\n",
            "|2012|     77.6|\n",
            "|2014|    88.09|\n",
            "|2013|    81.37|\n",
            "|2015|    90.97|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#What is the max High per year?\n",
        "\n",
        "#split the column Date to year month and day \n",
        "\n",
        "#import split from pyspark\n",
        "\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "#extract the year from the date column\n",
        "\n",
        "year_df = new_df.withColumn('year', split(new_df['Date'], '-').getItem(0))\n",
        "\n",
        "\n",
        "#use the year to group by max of high per year\n",
        "\n",
        "year_df.groupBy(\"year\").max(\"High\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f2f587c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2f587c9",
        "outputId": "f0d23052-0242-4427-8fd3-f217edce0dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|   07|74.43971962616824|\n",
            "|   11|72.11108910891085|\n",
            "|   01|71.44801980198022|\n",
            "|   09|72.18411764705883|\n",
            "|   05|72.30971698113206|\n",
            "|   08|73.02981818181819|\n",
            "|   03|71.77794392523363|\n",
            "|   02|71.30680412371134|\n",
            "|   06|72.49537735849057|\n",
            "|   10|71.57854545454546|\n",
            "|   12|72.84792452830189|\n",
            "|   04|72.97361904761907|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#What is the average Close for each Calendar Month?\n",
        "\n",
        "#create a column for month.\n",
        "calendar_month = new_df.withColumn('month', split(new_df['Date'], '-').getItem(1)) \n",
        "\n",
        "#calendar_month.show()\n",
        "\n",
        "#group by the monthly average of close\n",
        "calendar_month.groupBy(\"month\").avg(\"Close\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4943bf",
      "metadata": {
        "id": "af4943bf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}